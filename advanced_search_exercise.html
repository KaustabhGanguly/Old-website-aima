<!DOCTYPE HTML>
<html>
	<head>
		<title>Exercises</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="subpage">

		<!-- Header -->
			<header id="header" class="alt">
				<div class="logo"><a href="index.html">AIMA <span>Exercises</span></a></div>
				<a href="#menu"><span>Menu</span></a>
			</header>

		<!-- Nav -->
			<nav id="menu">
				<ul class="links">
					<li><a href="index.html">Home</a></li>
					<li><a href="info.html">More Info</a></li>
					<li><a href="exercises.html">Exercises</a></li>
				</ul>
			</nav>
		<!-- Main -->
			<div id="main" class="container" style = "text-align: left;">

				<!-- Content -->
				        <h5><b>EXERCISES</b></h1>
                <h1 id="advancedsearchexercises">Advanced Search Exercises</h1>

<h2 id="41localsearchalgorithmsandoptimizationproblems4exercises1labelled">4.1: Local Search Algorithms and Optimization Problems (4 exercises, 1 labelled)</h2>

<p><h1>1.</h1> Give the name of the algorithm that results from each of the following special cases: <br />
 a. Local beam search with k = 1.
 b. Local beam search with one initial state and no limit on the number of states retained.
 c. Simulated annealing with T = 0 at all times (and omitting the termination test).
 d. Simulated annealing with T = ∞ at all times.
 e. Genetic algorithm with population size N = 1.
 (id=4.0 section=4.1)</p>
				<div class="dropdownA">
					<span><h1>Hover for Answer</h1></span>
  						<div class="dropdown-contentA">

<p>a.) Local beam search with k = 1.</p>

<p>&ldquo;The local beam search algorithm keeps track of k states rather than just one. It begins with k randomly generated states. At each step, all the successors of all k states are generated. If any one is a goal, the algorithm halts. Otherwise, it selects the k best successors from the complete list and repeats&rdquo; (Russell &amp; Norvig, 125-126). A local beam search with k =1 is hill-climbing search. The hill-climbing search algorithm &ldquo;is simply a loop that continually moves in the direction of increasing value &ndash; that is, uphill. It terminates when it reaches a &ldquo;peak&rdquo; where no neighbor has a higher value&rdquo; (Russell &amp; Norvig, 122).</p>

<p>Since k=1 there is only 1 neighbor, and so only one choice to go.</p>

<p>b.) Local beam search with one initial state and no limit on the number of states retained.</p>

<p>Local beam search with k = &#8734;, with no limit on the number of states retained resembles a breadth-first search. &ldquo;Breadth-first search is a simple strategy in which the root node is expanded first, then all the successors of the root node are expanded next, then their successors, and so on&rdquo; (Russell &amp; Norvig, 81). Since each state is retained it will know the nodes that branch out to the following nodes all the way down the tree. This becomes a tree, retaining all states that have been investigated in the past.</p>

<p>c.) Simulated annealing with T = 0 at all times (and omitting the termination test).</p>

<p>Simulated annealing with T = 0 at all times, omitting the termination test, would resemble first-choice hill climbing. &ldquo;First-choice hill climbing implements stochastic hill climbing by generating successors randomly until one is generated that is better than the current state&rdquo; (Russell &amp; Norvig, 124).</p>

<p>d.) Simulated annealing with T = &#8734; at all times.</p>

<p>Simulated annealing with T = &#8734;, ignoring the fact that the termination step would never be triggered, the search would be identical to a random walk. &ldquo;A random walk simply selects at random one of the available actions from the current state; preference can be given to actions that have not yet been tried&rdquo; (Russell &amp; Norvig, 150-151).</p>

<p>e.) Genetic algorithm with population size N = 1.</p>

<p>&ldquo;A genetic algorithm is a variant of stochastic beam search in which successor states are generated by combining two parent states rather than by modifying a single state&rdquo; (Russell &amp; Norvig, 126). If N = 1, then two parent states can not be selected, it would be the same parent selected twice. Without having crossover to create new offspring with a mixture of the two parents, you will keep getting the same parent over and over again.</p>
							
			  		</div>
				</div>

				<p><h1>2.</h1> Exercise 3.16( will insert a link later ) considers the problem of building railway tracks under the assumption that pieces fit exactly with no slack. Now consider the real problem, in which pieces don’t fit exactly but allow for up to 10 degrees of rotation to either side of the “proper” alignment. Explain how to formulate the problem so it could be solved by simulated annealing.
 (id=4.1 section=4.1.2)</p><br/>
	
	<div class="dropdownA">
					<span><h1>Hover for Answer</h1></span>
  						<div class="dropdown-contentA">

<p>Despite its humble origins, this question raises many of the same issues as the scientifically important problem of protein design. There is a <b>discrete assembly space</b> in which pieces
are chosen to be added to the track and a continuous configuration space determined by the “joint angles” at every place where two pieces are linked. Thus we can define a state as a set of oriented, linked pieces and the associated joint angles in the <i>range [−10, 10]</i>, plus a set of unlinked pieces. The linkage and joint angles exactly determine the physical layout of the track; we can allow for (and penalize) layouts in which tracks lie on top of one another, or we can disallow them. The evaluation function would include terms for how many pieces are used, how many loose ends there are, and (if allowed) the degree of overlap. We might include a penalty for the amount of deviation from 0-degree joint angles. (We could also include terms for “interestingness” and “traversability”—for example, it is nice to be able to drive a train starting from any track segment to any other, ending up in either direction without having to lift up the train.) The tricky part is the set of allowed moves. Obviously we can unlink any piece or link an unlinked piece to an open peg with either orientation at any allowed angle (possibly excluding moves that create overlap). More problematic are moves to join a peg and hole on already-linked pieces and moves to change the angle of a joint. Changing one angle may force changes in others, and the changes will vary depending on whether the other pieces are at their joint-angle limit. In general <i>there will be no unique “minimal” solution for a given angle change in terms of the consequent changes to other angles, and some changes
may be impossible.</i>	</p>						
			  		</div>
				</div>

	<p><h1>3.</h1> In this exercise, we explore the use of local search methods to solve TSPs of the type defined in Exercise 3.30 ( will insert link later ).
    a. Implement and test a hill-climbing method to solve TSPs. Compare the results with optimal solutions obtained from the A* algorithm with the MST heuristic (Exercise 3.30).
    b. Repeat part (a) using a genetic algorithm instead of hill climbing. You may want to
 consult Larran ̃aga et al. (1999) for some suggestions for representations.
 (id=4.10 section=4.1)</p>

<div class="dropdownA">
					<span><h1>Hover for Answer</h1></span>
  						<div class="dropdown-contentA">

<p> Here is one simple hill-climbing algorithm:<br />
• Connect all the cities into an arbitrary path.<br />
• Pick two points along the path at random.<br />
• Split the path at those points, producing three pieces.<br />
• Try all six possible ways to connect the three pieces.<br />
• Keep the best one, and reconnect the path accordingly.<br />
• Iterate the steps above until no improvement is observed for a while </p>						
			  		</div>
				</div>

<p><h1>4.</h1> <i>[hill-climbing-exercise]%</i>:
 Generate a large number of 8-puzzle and 8-queens instances and solve them (where possible) by hill climbing (steepest-ascent and first-choice variants), hill climbing with random restart, and simulated annealing. Measure the search cost and percentage of solved problems and graph these against the optimal solution cost. Comment on your results
 (id=4.11 section=4.1)</p>

<div class="dropdownA">
					<span><h1>Hover for Answer</h1></span>
  						<div class="dropdown-contentA">

<p> Coming Soon </p>						
			  		</div>
				</div>

<h2 id="43searchingwithnondeterministicactions2exercises2labelled">4.3: Searching with Nondeterministic Actions (2 exercises, 2 labelled)</h2>

<p><h1>5.</h1> <i>[cond-plan-repeated-exercise]</i>:
 The AND-OR-GRAPH-SEARCH algorithm in : <img src="https://image.ibb.co/myHT87/Screen_Shot_2018_02_18_at_4_04_44_PM.png" alt="down" />
 checks for repeated states only on the path from the root to the current state. Suppose that, in addition, the algorithm were to store every visited state and check against that list. (See BREADTH-FIRST-SEARCH in Figure 3.11 from book for an example.) Determine the information that should be stored and how the algorithm should use that information when a repeated state is found. (Hint: You will need to distinguish at least between states for which a successful subplan was constructed previously and states for which no subplan could be found.) Explain how to use labels, as defined in Section 4.3.3, to avoid having multiple copies of subplans.
 (id=4.2 section=4.3.2)</p>

<div class="dropdownA">
					<span><h1>Hover for Answer</h1></span>
  						<div class="dropdown-contentA">

<p>See the figure :<br /><img src="https://image.ibb.co/hMnKyH/Screen_Shot_2018_02_28_at_7_12_37_PM.png" alt="down" />
	<br />. For states that OR-SEARCH finds a solution
for it records the solution found. If it later visits that state again it immediately returns that
solution.<br />
When OR-SEARCH fails to find a solution it has to be careful. Whether a state can be
solved depends on the path taken to that solution, as we do not allow cycles. So on failure
OR-SEARCH records the value of path. If a state is which has previously failed when path
contained any subset of its present value, OR-SEARCH returns failure.<br />
To avoid repeating sub-solutions we can label all new solutions found, record these
labels, then return the label if these states are visited again. Post-processing can prune off
unused labels. Alternatively, we can output a direct acyclic graph structure rather than a tree.
See <a href = "https://homes.cs.washington.edu/~weld/papers/mausam-ijcai07.pdf">(Bertoli et al., 2001)</a> for further details. </p>						
			  		</div>
				</div>

	<p><h1>6.</h1> <i>[cond-loop-exercise]</i>:
 Explain precisely how to modify the AND-OR-GRAPH-SEARCH algorithm to generate a cyclic plan if no acyclic plan exists. You will need to deal with three issues: labeling the plan steps so that a cyclic plan can point back to an earlier part of the plan, modifying OR-SEARCH so that it continues to look for acyclic plans after finding a cyclic plan, and augmenting the plan representation to indicate whether a plan is cyclic. Show how your algorithm works on (a) the slippery vacuum world, and (b) the slippery, erratic vacuum world. You might wish to use a computer implementation to check your results
 (id=4.3 section=4.3.3)</p>

<div class="dropdownA">
					<span><h1>Hover for Answer</h1></span>
  						<div class="dropdown-contentA">

<p>The question statement describes the required changes in detail,<br /><img src="https://image.ibb.co/gUAYQx/Screen_Shot_2018_02_28_at_7_13_48_PM.png" alt="down" /><br /> for the
modified algorithm. When OR-SEARCH cycles back to a state on path it returns a token loop
which means to loop back to the most recent time this state was reached along the path to
it. Since path is implicitly stored in the returned plan, there is sufficient information for later
processing, or a modified implementation, to replace these with labels.
The plan representation is implicitly augmented to keep track of whether the plan is
cyclic (i.e., contains a loop) so that OR-SEARCH can prefer acyclic solutions.
AND-SEARCH returns failure if all branches lead directly to a loop, as in this case the
plan will always loop forever. This is the only case it needs to check as if all branches in a
finite plan loop there must be some And-node whose children all immediately loop. </p>						
			  		</div>
				</div>

<h2 id="44searchingwithpartialobservations5exercises4labelled">4.4: Searching with Partial Observations (5 exercises, 4 labelled)</h2>

<p><h1>7.</h1> In Section 4.4.1 we introduced belief states to solve sensorless search problems. A sequence of actions solves a sensorless problem if it maps every physical state in the initial belief state <i>b</i> to a goal state. Suppose the agent knows h*(s), the true optimal cost of solving the physical state <i>s</i> in the fully observable problem, for every state <i>s</i> in <i>b</i>. Find an admissible heuristic <i>h(b)</i> for the sensorless problem in terms of these costs, and prove its admissibilty. Comment on the accuracy of this heuristic on the sensorless vacuum problem of : <br />
<img src="https://image.ibb.co/gvmzFn/Screen_Shot_2018_02_18_at_4_29_31_PM.png" alt="down" /><br />
How well does : A∗ perform?<br />
 id=4.4 section=4.4.1)</p>

<div class="dropdownA">
					<span><h1>Hover for Answer</h1></span>
  						<div class="dropdown-contentA">

<p>A sequence of actions is a solution to a belief state problem if it takes every initial
	physical state to a goal state. We can relax this problem by requiring it take only <i>some</i> initial
physical state to a goal state. To make this well defined, we’ll require that it finds a solution for the physical state with the most costly solution. If h∗(s) is the optimal cost of solution
	starting from the physical state <i>s</i>, then <br />h(S) = max h∗(s) where s∈S <br />is the heuristic estimate given by this relaxed problem. This heuristic assumes any solution
to the most difficult state the agent things possible will solve all states.<br />
On the sensorless vacuum cleaner problem in Figure 4.14, h correctly determines the
optimal cost for all states except the central three states (those reached by [suck], [suck, lef t]
and [suck, right]) and the root, for which h estimates to be 1 unit cheaper than they really
are. This  means A∗ will expand these three central nodes, before marching towards the
solution. </p>						
			  		</div>
				</div>

<p><h1>8.</h1> <i>[belief-state-superset-exercise]</i> :
  This exercise explores subset–superset relations between belief states in sensorless or partially observable environments.
 a. Prove that if an action sequence is a solution for a belief state b, it is also a solution for any subset of b. Can anything be said about supersets of b?
 b. Explain in detail how to modify graph search for sensorless problems to take advantage of your answers in (a).
 c. Explain in detail how to modify AND–OR search for partially observable problems, beyond the modifications you describe in (b).
 ( id=4.5 section=4.4)</p>

<div class="dropdownA">
					<span><h1>Hover for Answer</h1></span>
  						<div class="dropdown-contentA">

							<p><h1>a</h1>. An action sequence is a solution for belief state b if performing it starting in any state
	<i>s ∈ b</i> reaches a goal state. Since any state in a subset of <i>b</i> is in <i>b</i>, the result is immediate.<br />
Any action sequence which is not a solution for belief state b is also not a solution
for any superset; this is the contrapositive of what we’ve just proved. One cannot, in
general, say anything about arbitrary supersets, as the action sequence need not lead to
	a goal on the states outside of <i>b</i>. One can say, for example, that if an action sequence solves a belief state <i>b</i> and a belief state <i>b′</i> then it solves the union belief state <i>b ∪ b′</i>
.<br />
							<h1>b.</h1> On expansion of a node, do not add to the frontier any child belief state which is a
superset of a previously explored belief state.<br />
							<h1>c.</h1> If you keep a record of previously solved belief states, add a check to the start of ORsearch
to check whether the belief state passed in is a subset of a previously solved
belief state, returning the previous solution in case it is.</p>						
			  		</div>
				</div>

<p><h1>9.</h1> <i>[multivalued-sensorless-exercise]</i> :
On page 139 it was assumed that a given action would have the same cost when ex- ecuted in any physical state within a given belief state. (This leads to a belief-state search problem with well-defined step costs.) Now consider what happens when the assumption does not hold. Does the notion of optimality still make sense in this context, or does it require modification? Consider also various possible definitions of the “cost” of executing an action in a belief state; for example, we could use the <i>minimum</i> of the physical costs; or the <i>maximum</i>; or a cost <i>interval</i> with the lower bound being the minimum cost and the upper bound being the maximum; or just keep the set of all possible costs for that action. For each of these, explore whether A* (with modifications if necessary) can return optimal solutions
(id=4.6 section=4.4)</p>

<div class="dropdownA">
					<span><h1>Hover for Answer</h1></span>
  						<div class="dropdown-contentA">

<p>Consider a very simple example: an initial belief state {S1, S2}, actions a and b both
leading to goal state G from either initial state, and <br />
c(S1, a, G) = 3; c(S2, a, G) = 5;<br />
c(S1, b, G) = 2; c(S2, b, G) = 6 .<br />
In this case, the solution [a] costs 3 or 5, the solution [b] costs 2 or 6. Neither is “optimal” in
any obvious sense.<br />
In some cases, there will be an optimal solution. Let us consider just the deterministic
case. For this case, we can think of the cost of a plan as a mapping from each initial physical
state to the actual cost of executing the plan. In the example above, the cost for [a] is {S1:3, S2:5} and the cost for [b] is {S1:2, S2:6}. We can say that plan p1 weakly dominates
p2 if, for each initial state, the cost for p1 is no higher than the cost for p2. (Moreover, p1
dominates p2 if it weakly dominates it and has a lower cost for some state.) If a plan p weakly
dominates all others, it is optimal. Notice that this definition reduces to ordinary optimality in
the observable case where every belief state is a singleton. As the preceding example shows,
however, a problem may have no optimal solution in this sense. A perhaps acceptable version
of A∗ would be one that returns any solution that is not dominated by another. To understand whether it is possible to apply A∗ at all, it helps to understand its dependence on Bellman’s (1957) principle of optimality: An optimal policy has the property (PRINCIPLE OF OPTIMALITY ) that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision. It is important to
understand that this is a restriction on performance measures designed to facilitate efficient
algorithms, not a general definition of what it means to be optimal.
In particular, if we define the cost of a plan in belief-state space as the minimum cost
of any physical realization, we violate Bellman’s principle. Modifying and extending the
previous example, suppose that a and b reach S3 from S1 and S4 from S2, and then reach G
from there:<br />
c(S1, a, S3)= 6; c(S2, a, S4)= 2;<br />
c(S1, b, S3)= 6; c(S2, b, S4)=1 .c(S3, a, G) = 2; c(S4, a, G) = 2;<br />
c(S3, b, G) = 1; c(S4, b, G)=9 .<br />
In the belief state {S3, S4}, the minimum cost of [a] is min{2, 2} = 2 and the minimum cost
of [b] is min{1, 9} = 1, so the optimal plan is [b]. In the initial belief state {S1, S2}, the four
possible plans have the following costs:<br />
[a, a] : min{8, 4} = 4;[a, b] : min{7, 11} = 7;[b, a] : min{8, 3} = 3;[b, b] : min{7, 10} = 7 .<br />
Hence, the optimal plan in {S1, S2} is [b, a], which does not choose b in {S3, S4} even though
that is the optimal plan at that point. This counterintuitive behavior is a direct consequence
of choosing the minimum of the possible path costs as the performance measure.<br />
This example gives just a small taste of what might happen with nonadditive performance
measures. Details of how to modify and analyze A∗ for general path-dependent cost
functions are give by Dechter and Pearl (1985). Many aspects of A∗ carry over; for example,
we can still derive lower bounds on the cost of a path through a given node. For a belief state
b, the minimum value of <b>g(s) + h(s)</b> for each state s in b is a lower bound on the minimum
cost of a plan that goes through b.</p>						
			  		</div>
				</div>

<p><h1>10.</h1> <i>[vacuum-solvable-exercise]</i> :
 Consider the sensorless version of the erratic vacuum world. Draw the belief-state space reachable from the initial belief state {1, 2, 3, 4, 5, 6, 7, 8}, and explain why the problem is unsolvable.
 (id=4.7 section=4.4)</p>

<div class="dropdownA">
					<span><h1>Hover for Answer</h1></span>
  						<div class="dropdown-contentA">

<p> <img src="https://image.ibb.co/h0W7kx/Screen_Shot_2018_02_28_at_7_16_08_PM.png" alt="down" /><br />
The belief state space is shown in Figure S4.3. No solution is possible because no path
leads to a belief state all of whose elements satisfy the goal. If the problem is fully observable,
the agent reaches a goal state by executing a sequence such that Suck is performed only in a
dirty square. This ensures deterministic behavior and every state is obviously solvable.</p>						
			  		</div>
				</div>

<p><h1>11.</h1> <i>[vacuum-solvable-exercise]</i> :
 Consider the sensorless version of the erratic vacuum world .
Draw the belief-state space reachable from the initial belief state 
N{1,3,5,7} , and explain why the problem is unsolvable.
 ( id=4.7 section=4.4 )</p>

<div class="dropdownA">
					<span><h1>Hover for Answer</h1></span>
  						<div class="dropdown-contentA">

<p>Coming Soon </p>						
			  		</div>
				</div>

<h2 id="45onlinesearchagentsandunknownenvironments4exercises2labelled">4.5: Online Search Agents and Unknown Environments (4 exercises, 2 labelled)</h2>

<p><h1>12.</h1> <i>[online-offline-exercise]</i> :
 Suppose that an agent is in a 3×3 maze environment like the one shown in Figure : <br />
<img src="https://image.ibb.co/dNaJan/Screen_Shot_2018_02_18_at_4_47_14_PM.png" alt="down" /><br />
The agent knows that its initial location is (1,1), that the goal is at (3,3), and that the actions <i>Up, Down, Left, Right </i>have their usual effects unless blocked by a wall. The agent does not know where the internal walls are. In any given state, the agent perceives the set of legal actions; it can also tell whether the state is one it has visited before.
a. Explain how this online search problem can be viewed as an offline search in belief-state space, where the initial belief state includes all possible environment configurations. How large is the initial belief state? How large is the space of belief states?
b. How many distinct percepts are possible in the initial state?
c. Describe the first few branches of a contingency plan for this problem. How large
(roughly) is the complete plan?
Notice that this contingency plan is a solution for <i>every possible environment </i> fitting the given description. Therefore, interleaving of search and execution is not strictly necessary even in unknown environments.
(id=4.8 section=4.5.1)</p>

<div class="dropdownA">
					<span><h1>Hover for Answer</h1></span>
  						<div class="dropdown-contentA">

<p><img src="https://image.ibb.co/k4eeXc/Screen_Shot_2018_02_28_at_7_29_39_PM.png" alt="down" /><br />
This question is slightly ambiguous as to what the percept is—either the percept is just
the location, or it gives exactly the set of unblocked directions (i.e., blocked directions are
illegal actions). We will assume the latter. (Exercise may be modified in future printings.)
There are 12 possible locations for internal walls, so there are 2^12 = 4096 possible environment
configurations. A belief state designates a subset of these as possible configurations;
for example, before seeing any percepts all 4096 configurations are possible—this is a single
belief state. <br > 
							a. Online search is equivalent to offline search in belief-state space where each action
in a belief-state can have multiple successor belief-states: one for each percept the
agent could observe after the action. A successor belief-state is constructed by taking
the previous belief-state, itself a set of states, replacing each state in this belief-state
by the successor state under the action, and removing all successor states which are
inconsistent with the percept. This is exactly the construction in Section 4.4.2. AND-OR
search can be used to solve this search problem. The initial belief state has 2^10 = 1024
states in it, as we know whether two edges have walls or not (the upper and right edges
have no walls) but nothing more. There are 2^(2^12) possible belief states, one for each set
of environment configurations.<br />
We can view this as a contingency problem in belief state space. After each action
and percept, the agent learns whether or not an internal wall exists between the
current square and each neighboring square. Hence, each reachable belief state can be
represented exactly by a list of status values (present, absent, unknown) for each wall
separately. That is, the belief state is completely decomposable and there are exactly 3^12
reachable belief states. The maximum number of possible wall-percepts in each state
is 16 (2^4), so each belief state has four actions, each with up to 16 nondeterministic
successors.<br />
b. Assuming the external walls are known, there are two internal walls and hence 22 = 4
possible percepts.<br />
c. The initial null action leads to four possible belief states, as shown in Figure S4.4. From
each belief state, the agent chooses a single action which can lead to up to 8 belief states
(on entering the middle square). Given the possibility of having to retrace its steps at
a dead end, the agent can explore the entire maze in no more than 18 steps, so the
complete plan (expressed as a tree) has no more than 8^18 nodes. On the other hand,
there are just 3^12 reachable belief states, so the plan could be expressed more concisely
as a table of actions indexed by belief state (a policy in the terminology of Chapter 17).</p>						
			  		</div>
				</div>
						


<p><h1>13.</h1> <i>[path-planning-agent-exercise]</i> :
 We can turn the navigation problem in Exercise 3.7 into an environment as follows:</p>



<p align="left">The percept will be a list of the positions, relative to the agent, of the visible vertices. The percept does not include the position of the robot! The robot must learn its own po- sition from the map; for now, you can assume that each location has a different “view.”</p>

<p align="left">Each action will be a vector describing a straight-line path to follow. If the path is unobstructed, the action succeeds; otherwise, the robot stops at the point where its path first intersects an obstacle. If the agent returns a zero motion vector and is at the goal (which is fixed and known), then the environment teleports the agent to a random location (not inside an obstacle).</p>

<p align="left">The performance measure charges the agent 1 point for each unit of distance traversed and awards 1000 points each time the goal is reached.</p>

<p align="left">a) Implement this environment and a problem-solving agent for it. After each teleportation, the agent will need to formulate a new problem, which will involve discovering its current location.</p>

<p align="left">b) Document your agent’s performance (by having the agent generate suitable commentary as it moves around) and report its performance over 100 episodes.</p>

<p align="left">c) Modify the environment so that 30% of the time the agent ends up at an unintended destination (chosen randomly from the other visible vertices if any; otherwise, no move at all). This is a crude model of the motion errors of a real robot. Modify the agent so that when such an error is detected, it finds out where it is and then constructs a plan to get back to where it was and resume the old plan. Remember that sometimes getting back to where it was might also fail! Show an example of the agent successfully overcoming two successive motion errors and still reaching the goal.</p>

<p align="left">d) Now try two different recovery schemes after an error: (a) head for the closest vertex on the original route; and (b) replan a route to the goal from the new location. Compare the performance of the three recovery schemes. Would the inclusion of search costs affect the comparison?</p>

<p align="left">e) Now suppose that there are locations from which the view is identical. (For example, suppose the world is a grid with square obstacles.) What kind of problem does the agent now face? What do solutions look like?</p>


<p align="left">( id=4.9 section=4.4)</p>

<div class="dropdownA">
					<span><h1>Hover for Answer</h1></span>
  						<div class="dropdown-contentA">

<p>The student needs to make several design choices in answering this question. First,
how will the vertices of objects be represented? The problem states the percept is a list of
vertex positions, but that is not precise enough. Here is one good choice: The agent has an orientation (a heading in degrees). The visible vertexes are listed in clockwise order, starting
straight ahead of the agent. Each vertex has a relative angle (0 to 360 degrees) and a distance.<br />
We also want to know if a vertex represents the left edge of an obstacle, the right edge, or an
interior point. We can use the symbols L, R, or I to indicate this.<br />
The student will need to do some basic computational geometry calculations: intersection
of a path and a set of line segments to see if the agent will bump into an obstacle, and
visibility calculations to determine the percept. There are efficient algorithms for doing this
on a set of line segments, but don’t worry about efficiency; an exhaustive algorithm is ok. If
this seems too much, the instructor can provide an environment simulator and ask the student
only to program the agent.<br />
To answer (c), the student will need some exchange rate for trading off search time with
movement time. It is probably too complex to make the simulation asynchronous real-time;
easier to impose a penalty in points for computation.<br />
For (d), the agent will need to maintain a set of possible positions. Each time the agent
moves, it may be able to eliminate some of the possibilities. The agent may consider moves
that serve to reduce uncertainty rather than just get to the goal. </p>						
			  		</div>
				</div>

<p><h1>14</h1><i>[path-planning-hc-exercise]</i> :
 In this exercise, we examine hill climbing in the context of robot navigation, using the environment in figure : <br /><img src="https://image.ibb.co/e63Gvn/Screen_Shot_2018_02_18_at_8_24_52_PM.png" alt="down" /><br />as an example.
    a. Repeat above Exercise using hill climbing. Does your agent ever get stuck in a local minimum? Is it <i>possible</i> for it to get stuck with convex obstacles?
    b. Construct a nonconvex polygonal environment in which the agent gets stuck.
    c. Modify the hill-climbing algorithm so that, instead of doing a depth-1 search to decide where to go next, it does a depth-<i>k</i> search. It should find the best <i>k</i>-step path and do one step along it, and then repeat the process.
    d. Is there some k for which the new algorithm is guaranteed to escape from local minima?
    e. Explain how LRTA∗ enables the agent to escape from local minima in this case.
    (id=4.12 section=4.5)</p>

<div class="dropdownA">
					<span><h1>Hover for Answer</h1></span>
  						<div class="dropdown-contentA">

<p><img src="https://image.ibb.co/hdhf5x/Screen_Shot_2018_02_28_at_7_18_21_PM.png" alt="down" /><br />Hillclimbing is surprisingly effective at finding reasonable if not optimal paths for very
little computational cost, and seldom fails in two dimensions. <br />
	a. It is possible (see Figure S4.5(a)) but very unlikely—the obstacle has to have an unusual
shape and be positioned correctly with respect to the goal.<br />
b. With nonconvex obstacles, getting stuck is much more likely to be a problem (see Figure
S4.5(b)).<br />
c. Notice that this is just depth-limited search, where you choose a step along the best path
even if it is not a solution.<br />
	d. Set <i>k</i> to the maximum number of sides of any polygon and you can always escape.<br />
e. LRTA* always makes a move, but may move back if the old state looks better than the
new state. But then the old state is penalized for the cost of the trip, so eventually the
local minimum fills up and the agent escapes.  </p>						
			  		</div>
				</div>

<p><h1>15.</h1> Like DFS, online DFS is incomplete for reversible state spaces with infinite paths. For  example, suppose that states are points on the infinite two-dimensional grid and actions  are unit vectors (1, 0), (0, 1), (−1, 0), (0, −1), tried in that order. Show that online DFS  starting at (0, 0) will not reach (1, −1). Suppose the agent can observe, in addition to its  current state, all successor states and the actions that would lead to them. Write an  algorithm that is complete even for bidirected state spaces with infinite paths. What states  does it visit in reaching (1, −1)?
(id=4.13 section=4.5)</p>

<div class="dropdownA">
					<span><h1>Hover for Answer</h1></span>
  						<div class="dropdown-contentA">

<p>Since we can observe successor states, we always know how to backtrack from to a
previous state. This means we can adapt iterative deepening search to solve this problem.
The only difference is backtracking must be explicit, following the action which the agent
can see leads to the previous state.<br />
The algorithm expands the following nodes:<br />
Depth 1: (0, 0), (1, 0), (0, 0), (−1, 0), (0, 0)<br />
Depth 2: (0, 1), (0, 0), (0, −1), (0, 0), (1, 0), (2, 0), (1, 0), (0, 0), (1, 0), (1, 1), (1, 0), (1, −1) </p>						
			  		</div>
				</div>

<p><h1>16.</h1> Relate the time complexity of LRTA* to its space complexity.
(id=4.14 section=4.5)</p>

<div class="dropdownA">
					<span><h1>Hover for Answer</h1></span>
  						<div class="dropdown-contentA">

<p>Coming Soon </p>						
			  		</div>
				</div>

<p><h1>17.</h1> <i>{safe-ratio-exercise}</i>
  Extend the state spaces in <br /><img src="https://image.ibb.co/ksYfo7/Screen_Shot_2018_02_18_at_8_35_56_PM.png" alt="down" /> <br />to make them safely explorable, and prove that no bounded competitive ratio can be guaranteed in safely explorable environments.</p>

<div class="dropdownA">
					<span><h1>Hover for Answer</h1></span>
  						<div class="dropdown-contentA">

<p>Coming Soon </p>						
			  		</div>
				</div>
                
				        
		<!-- Footer -->
			<footer id="footer">
				<div class="inner" >

					<ul class="icons">
						<li><a href="https://www.twitter.com/pnorvig?lang=en" class="icon round fa-twitter"><span class="label">Twitter</span></a></li>
						<li><a href="https://www.github.com/aimacode/aima-exercises" class="icon round fa-github"><span class="label">Facebook</span></a></li>
						<li><a href="https://www.facebook.com/peter.norvig" class="icon round fa-facebook"><span class="label">Instagram</span></a></li>
					</ul>

					<div class="copyright" >
						&copy; Peter Norvig . Design: <a href="www.github.com/kaustabhganguly" style="text-decoration:none; ">Kaustabh Ganguly</a>. Mentor: <a href="https://images.gr-assets.com/authors/1483785693p8/15468.jpg" style="text-decoration:none; ">Peter Norvig</a>.
					</div>

				</div>
			</footer>


		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
